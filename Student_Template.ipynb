{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRit5A9KCx20"
   },
   "outputs": [],
   "source": "# Importing the OpenAI library to interact with OpenAI's API services.\n# Import basic libraries\n\nimport os\nimport logging\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nfrom typing import Tuple, List, Dict, Optional\n\n# LangChain imports\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import Document\nfrom langchain.tools import tool\nfrom langchain.agents import AgentExecutor, create_openai_functions_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n# Gradio for UI\nimport gradio as gr\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================\n# LOGGING CONFIGURATION\n# ============================================================\n\n# Create logs directory if it doesn't exist\nLOG_DIR = \"logs\"\nos.makedirs(LOG_DIR, exist_ok=True)\n\n# Configure logging format\nLOG_FORMAT = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\nLOG_DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n# Create file handler for persistent logs\nlog_filename = os.path.join(LOG_DIR, f\"chatbot_{datetime.now().strftime('%Y%m%d')}.log\")\nfile_handler = logging.FileHandler(log_filename, encoding='utf-8')\nfile_handler.setLevel(logging.DEBUG)\nfile_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=LOG_DATE_FORMAT))\n\n# Create console handler for immediate feedback\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nconsole_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=LOG_DATE_FORMAT))\n\n# Create logger for the chatbot\nlogger = logging.getLogger(\"MovieChatbot\")\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(file_handler)\nlogger.addHandler(console_handler)\n\n# Prevent duplicate logs\nlogger.propagate = False\n\nlogger.info(\"=\" * 50)\nlogger.info(\"IMDb Movie Chatbot - Session Started\")\nlogger.info(\"=\" * 50)\n\nprint(\"All libraries imported successfully!\")\nprint(f\"Logging to: {log_filename}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKBjz45zDj6r"
   },
   "outputs": [],
   "source": "# Store your OpenAI API key\n# Option 1: Load from .env file (recommended for security)\nload_dotenv()\n\n# Option 2: Set directly (use for testing only - don't commit to version control)\n# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n\n# Verify API key is set\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif OPENAI_API_KEY:\n    print(\"OpenAI API key loaded successfully!\")\nelse:\n    print(\"WARNING: OPENAI_API_KEY not found. Please set it in .env file or environment variables.\")\n    print(\"Create a .env file with: OPENAI_API_KEY=your-key-here\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XiPvpp1FxKb"
   },
   "outputs": [],
   "source": "# Load the data\n# Dataset path - adjust if loading from Google Drive\nDATASET_PATH = \"IMDb_Dataset (1).csv\"\n\nlogger.info(f\"Loading dataset from: {DATASET_PATH}\")\n\n# Load the IMDb dataset\ntry:\n    df = pd.read_csv(DATASET_PATH)\n    logger.info(f\"Dataset loaded successfully: {df.shape[0]} movies, {df.shape[1]} features\")\nexcept FileNotFoundError:\n    logger.error(f\"Dataset file not found: {DATASET_PATH}\")\n    raise\nexcept Exception as e:\n    logger.error(f\"Error loading dataset: {str(e)}\")\n    raise\n\nprint(f\"Dataset loaded successfully!\")\nprint(f\"Shape: {df.shape[0]} movies, {df.shape[1]} features\")\nprint(f\"\\nColumns: {list(df.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MznuqvAmRx_g"
   },
   "outputs": [],
   "source": "# View & Understand the data\n\n# Basic info\nprint(\"=\" * 60)\nprint(\"DATASET OVERVIEW\")\nprint(\"=\" * 60)\n\n# First few rows\nprint(\"\\nüìä Sample Data (First 5 rows):\")\ndisplay(df.head())\n\n# Data types and non-null counts\nprint(\"\\nüìã Data Types & Missing Values:\")\nprint(df.info())\n\n# Statistical summary for numerical columns\nprint(\"\\nüìà Statistical Summary (Numerical):\")\ndisplay(df.describe())\n\n# Missing values analysis\nprint(\"\\n‚ö†Ô∏è Missing Values:\")\nmissing = df.isnull().sum()\nmissing_pct = (missing / len(df) * 100).round(2)\nmissing_df = pd.DataFrame({'Missing Count': missing, 'Percentage': missing_pct})\ndisplay(missing_df[missing_df['Missing Count'] > 0])\n\n# Unique values for categorical columns\nprint(\"\\nüé≠ Unique Values:\")\nprint(f\"- Genres: {df['Genre'].nunique()} unique genres\")\nprint(f\"- Certificates: {df['Certificates'].nunique()} unique certificates\")\nprint(f\"- Directors: {df['Director'].nunique()} unique directors\")\nprint(f\"- Year range: {df['Year'].min()} - {df['Year'].max()}\")\n\n# Genre distribution (top 10)\nprint(\"\\nüé¨ Top 10 Genres:\")\nprint(df['Genre'].value_counts().head(10))\n\n# Rating distribution\nprint(\"\\n‚≠ê Rating Distribution:\")\nprint(f\"- Mean IMDb Rating: {df['IMDb Rating'].mean():.2f}\")\nprint(f\"- Median IMDb Rating: {df['IMDb Rating'].median():.2f}\")\nprint(f\"- Rating Range: {df['IMDb Rating'].min()} - {df['IMDb Rating'].max()}\")"
  },
  {
   "cell_type": "code",
   "source": "# Create movie description for each movie from the details provided in the dataset\n\ndef create_movie_description(row):\n    \"\"\"\n    Create a rich text description for each movie combining all available metadata.\n    This description will be used for embedding and retrieval.\n    \"\"\"\n    # Handle missing values\n    title = row['Title'] if pd.notna(row['Title']) else 'Unknown Title'\n    year = int(row['Year']) if pd.notna(row['Year']) else 'Unknown Year'\n    genre = row['Genre'] if pd.notna(row['Genre']) else 'Unknown Genre'\n    director = row['Director'] if pd.notna(row['Director']) else 'Unknown Director'\n    cast = row['Star Cast'] if pd.notna(row['Star Cast']) else 'Unknown Cast'\n    rating = row['IMDb Rating'] if pd.notna(row['IMDb Rating']) else 'N/A'\n    metascore = row['MetaScore'] if pd.notna(row['MetaScore']) else 'N/A'\n    certificate = row['Certificates'] if pd.notna(row['Certificates']) else 'Not Rated'\n    duration = int(row['Duration (minutes)']) if pd.notna(row['Duration (minutes)']) else 'Unknown'\n    poster = row['Poster-src'] if pd.notna(row['Poster-src']) else ''\n    \n    # Create structured description\n    description = f\"\"\"\nMovie Title: {title}\nYear: {year}\nGenre: {genre}\nDirector: {director}\nStar Cast: {cast}\nIMDb Rating: {rating}/10\nMetaScore: {metascore}\nCertificate: {certificate}\nDuration: {duration} minutes\nPoster URL: {poster}\n\nThis is a {genre} movie titled \"{title}\" released in {year}. \nIt was directed by {director} and stars {cast}. \nThe film has an IMDb rating of {rating}/10 and a MetaScore of {metascore}. \nIt is rated {certificate} with a runtime of {duration} minutes.\n\"\"\".strip()\n    \n    return description\n\n# Apply the function to create descriptions\nprint(\"Creating movie descriptions...\")\ndf['description'] = df.apply(create_movie_description, axis=1)\n\n# Preview a sample description\nprint(\"\\n‚úÖ Movie descriptions created successfully!\")\nprint(f\"\\nSample description for first movie:\\n\")\nprint(\"-\" * 60)\nprint(df['description'].iloc[0])\nprint(\"-\" * 60)\n\n# Show description statistics\nprint(f\"\\nDescription Statistics:\")\nprint(f\"- Total movies with descriptions: {len(df)}\")\nprint(f\"- Average description length: {df['description'].str.len().mean():.0f} characters\")",
   "metadata": {
    "id": "3UXD8fTl-D8X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Now, data is ready!\n# Its time to create your vector store\n# Perform Text Chunking\n\n# For movie data, each movie description is a natural document unit\n# We'll create Document objects with metadata for better retrieval\n\ndef create_documents_from_dataframe(df):\n    \"\"\"\n    Convert DataFrame rows to LangChain Document objects with metadata.\n    Each movie becomes a single document for optimal retrieval.\n    \"\"\"\n    documents = []\n    \n    for idx, row in df.iterrows():\n        # Create metadata for filtering and display\n        metadata = {\n            'title': row['Title'] if pd.notna(row['Title']) else 'Unknown',\n            'year': int(row['Year']) if pd.notna(row['Year']) else 0,\n            'genre': row['Genre'] if pd.notna(row['Genre']) else 'Unknown',\n            'director': row['Director'] if pd.notna(row['Director']) else 'Unknown',\n            'rating': float(row['IMDb Rating']) if pd.notna(row['IMDb Rating']) else 0.0,\n            'certificate': row['Certificates'] if pd.notna(row['Certificates']) else 'Not Rated',\n            'poster_url': row['Poster-src'] if pd.notna(row['Poster-src']) else '',\n            'duration': int(row['Duration (minutes)']) if pd.notna(row['Duration (minutes)']) else 0,\n            'index': idx\n        }\n        \n        # Create Document with description as page_content\n        doc = Document(\n            page_content=row['description'],\n            metadata=metadata\n        )\n        documents.append(doc)\n    \n    return documents\n\n# Create documents\nprint(\"Converting movie data to documents...\")\ndocuments = create_documents_from_dataframe(df)\n\nprint(f\"\\n‚úÖ Created {len(documents)} documents\")\nprint(f\"\\nSample document:\")\nprint(\"-\" * 60)\nprint(f\"Content preview: {documents[0].page_content[:200]}...\")\nprint(f\"\\nMetadata: {documents[0].metadata}\")\nprint(\"-\" * 60)\n\n# Optional: Use text splitter for very long documents\n# For our movie descriptions, this isn't needed, but shown for completeness\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    length_function=len,\n)\n\n# Check if any documents need splitting\nlong_docs = [d for d in documents if len(d.page_content) > 1000]\nprint(f\"\\nDocuments exceeding 1000 chars: {len(long_docs)}\")\n\n# For this dataset, we'll use documents as-is (each movie = 1 document)\n# If needed: split_documents = text_splitter.split_documents(documents)",
   "metadata": {
    "id": "1tCJAI89-REL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create embeddings for the chunks\n# See https://python.langchain.com/docs/integrations/text_embedding/ for available models\n\n# Initialize OpenAI Embeddings\n# Using text-embedding-ada-002 (default) or text-embedding-3-small for better performance\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",  # Cost-effective and performant\n    # model=\"text-embedding-ada-002\",  # Alternative option\n)\n\nprint(\"‚úÖ OpenAI Embeddings model initialized\")\nprint(f\"   Model: text-embedding-3-small\")\n\n# Test embedding on a sample text\nsample_text = \"Action movie with car chases\"\nsample_embedding = embeddings.embed_query(sample_text)\n\nprint(f\"\\nüìê Embedding dimensions: {len(sample_embedding)}\")\nprint(f\"   Sample embedding preview: [{sample_embedding[0]:.6f}, {sample_embedding[1]:.6f}, ...]\")",
   "metadata": {
    "id": "hUutqmZF-jf4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSlO9vblWGDt"
   },
   "outputs": [],
   "source": "# Create a vector store using the created chunks and the embeddings model\n\nVECTORSTORE_PATH = \"imdb_vectorstore\"\n\n# Check if vector store already exists (faster loading on re-run)\nif os.path.exists(VECTORSTORE_PATH):\n    logger.info(f\"Loading existing FAISS vector store from: {VECTORSTORE_PATH}\")\n    print(\"Loading existing FAISS vector store...\")\n    vectorstore = FAISS.load_local(\n        VECTORSTORE_PATH, \n        embeddings,\n        allow_dangerous_deserialization=True  # Required for loading\n    )\n    logger.info(f\"Loaded vector store with {vectorstore.index.ntotal} vectors\")\n    print(f\"‚úÖ Loaded existing vector store with {vectorstore.index.ntotal} vectors\")\nelse:\n    logger.info(f\"Creating new FAISS vector store from {len(documents)} documents\")\n    print(\"Creating FAISS vector store from movie documents...\")\n    print(f\"This may take a few minutes for {len(documents)} documents...\")\n    \n    # Create FAISS vector store from documents\n    start_time = time.time() if 'time' in dir() else None\n    vectorstore = FAISS.from_documents(\n        documents=documents,\n        embedding=embeddings\n    )\n    \n    logger.info(f\"Vector store created with {vectorstore.index.ntotal} vectors\")\n    print(f\"\\n‚úÖ FAISS vector store created successfully!\")\n    print(f\"   Total vectors: {vectorstore.index.ntotal}\")\n    \n    # Save vector store locally for faster loading next time\n    vectorstore.save_local(VECTORSTORE_PATH)\n    logger.info(f\"Vector store saved to: {VECTORSTORE_PATH}\")\n    print(f\"   Vector store saved to: {VECTORSTORE_PATH}/\")\n\n# Test similarity search\nlogger.debug(\"Testing similarity search...\")\nprint(\"\\nüîç Testing similarity search...\")\ntest_query = \"comedy movie with Jim Carrey\"\nsimilar_docs = vectorstore.similarity_search(test_query, k=3)\n\nprint(f\"\\nQuery: '{test_query}'\")\nprint(f\"Top 3 results:\")\nfor i, doc in enumerate(similar_docs, 1):\n    print(f\"\\n{i}. {doc.metadata['title']} ({doc.metadata['year']})\")\n    print(f\"   Genre: {doc.metadata['genre']} | Rating: {doc.metadata['rating']}\")\n\nlogger.debug(f\"Similarity search test completed: {len(similar_docs)} results\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dJXzILCXStu"
   },
   "outputs": [],
   "source": "# Create the llm model\n\n# Initialize ChatOpenAI with GPT-4 or GPT-3.5-turbo\n# Note: streaming=True enables token-by-token streaming for real-time responses\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",  # Cost-effective option with good performance\n    # model=\"gpt-4o\",     # Higher quality but more expensive\n    # model=\"gpt-3.5-turbo\",  # Budget option\n    temperature=0.7,  # Balanced creativity\n    max_tokens=1000,\n    streaming=True,  # Enable streaming for real-time responses\n)\n\n# Create a non-streaming version for batch operations\nllm_batch = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=1000,\n    streaming=False,\n)\n\nlogger.info(\"LLM models initialized (streaming + batch)\")\n\nprint(\"‚úÖ LLM model initialized\")\nprint(f\"   Model: gpt-4o-mini\")\nprint(f\"   Temperature: 0.7\")\nprint(f\"   Max tokens: 1000\")\nprint(f\"   Streaming: Enabled\")\n\n# Test the LLM\ntest_response = llm_batch.invoke(\"Say hello in one sentence.\")\nprint(f\"\\nüß™ LLM test response: {test_response.content}\")"
  },
  {
   "cell_type": "code",
   "source": "# Create the prompt template\n\n# Define a comprehensive prompt template for movie recommendations\nMOVIE_PROMPT_TEMPLATE = \"\"\"You are an expert Movie Recommendation Assistant with access to the IMDb movie database. \nYour role is to help users discover movies based on their preferences and queries.\n\nUse the following movie information from our database to answer the user's question:\n\n{context}\n\nGuidelines:\n1. Only recommend movies from the provided context - do not make up movie information\n2. Provide relevant details like title, year, genre, director, cast, and ratings when available\n3. If the user asks for recommendations, suggest movies that match their criteria\n4. If no relevant movies are found in the context, politely say so\n5. Be conversational and helpful in your responses\n6. Format your response clearly with movie details\n\nUser Question: {question}\n\nHelpful Answer:\"\"\"\n\n# Create the PromptTemplate object\nprompt = PromptTemplate(\n    template=MOVIE_PROMPT_TEMPLATE,\n    input_variables=[\"context\", \"question\"]\n)\n\nprint(\"‚úÖ Prompt template created\")\nprint(\"\\nüìù Template preview:\")\nprint(\"-\" * 60)\nprint(MOVIE_PROMPT_TEMPLATE[:500] + \"...\")",
   "metadata": {
    "id": "RzhBYe1K_BCo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create the document processing chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.chains import create_retrieval_chain\n\n# Create a stuff documents chain (combines documents into context)\ncombine_docs_chain = create_stuff_documents_chain(\n    llm=llm,\n    prompt=prompt\n)\n\nprint(\"‚úÖ Document processing chain created\")\nprint(\"   Chain type: Stuff Documents Chain\")\nprint(\"   This chain combines retrieved documents into context for the LLM\")",
   "metadata": {
    "id": "jAiIUA1T_FPI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create a retriever from the vector store for fetching relevant documents\n# See https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/\n\n# Create retriever with configurable search parameters\nretriever = vectorstore.as_retriever(\n    search_type=\"similarity\",  # Options: \"similarity\", \"mmr\" (maximal marginal relevance)\n    search_kwargs={\n        \"k\": 5  # Number of documents to retrieve\n    }\n)\n\nprint(\"‚úÖ Retriever created from vector store\")\nprint(\"   Search type: Similarity\")\nprint(\"   Top-k results: 5\")\n\n# Create the full retrieval chain\nretrieval_chain = create_retrieval_chain(\n    retriever=retriever,\n    combine_docs_chain=combine_docs_chain\n)\n\nprint(\"\\n‚úÖ Full retrieval chain assembled\")\nprint(\"   Pipeline: Query ‚Üí Retriever ‚Üí Documents ‚Üí LLM ‚Üí Response\")",
   "metadata": {
    "id": "VMiWghjrl5oB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Invoke the retrieval chain to process the user's query\n\ndef ask_movie_bot(question: str) -> dict:\n    \"\"\"\n    Process a user question through the retrieval chain.\n    Returns the response and source documents.\n    \"\"\"\n    response = retrieval_chain.invoke({\"input\": question})\n    return response\n\n# Test with sample queries\ntest_queries = [\n    \"Recommend some good documentary movies\",\n    \"What are some biography films with high ratings?\",\n    \"Find movies directed by Christopher Nolan\",\n]\n\nprint(\"üé¨ Testing the Movie Chatbot\")\nprint(\"=\" * 60)\n\nfor query in test_queries:\n    print(f\"\\nüìù Query: {query}\")\n    print(\"-\" * 40)\n    \n    response = ask_movie_bot(query)\n    print(f\"\\nü§ñ Response:\\n{response['answer']}\")\n    \n    print(f\"\\nüìö Sources used: {len(response['context'])} documents\")\n    print(\"=\" * 60)",
   "metadata": {
    "id": "6vLrWAVj_LTv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Perform adequate formatting to print the final response in a user readable format\n\ndef format_movie_response(response: dict, show_sources: bool = False) -> str:\n    \"\"\"\n    Format the chatbot response in a user-friendly way.\n    \n    Args:\n        response: The response dict from retrieval chain\n        show_sources: Whether to include source movie information\n    \n    Returns:\n        Formatted string response\n    \"\"\"\n    output = []\n    \n    # Main answer\n    output.append(\"üé¨ Movie Bot Response:\")\n    output.append(\"=\" * 50)\n    output.append(response['answer'])\n    \n    # Optionally show source movies\n    if show_sources and 'context' in response:\n        output.append(\"\\n\" + \"-\" * 50)\n        output.append(\"üìö Movies referenced:\")\n        \n        for i, doc in enumerate(response['context'][:5], 1):\n            meta = doc.metadata\n            output.append(f\"\\n  {i}. {meta.get('title', 'N/A')} ({meta.get('year', 'N/A')})\")\n            output.append(f\"     Genre: {meta.get('genre', 'N/A')} | Rating: {meta.get('rating', 'N/A')}/10\")\n    \n    return \"\\n\".join(output)\n\n\ndef chat_with_bot(user_input: str, show_sources: bool = True) -> str:\n    \"\"\"\n    Main function to chat with the movie bot.\n    \"\"\"\n    response = ask_movie_bot(user_input)\n    return format_movie_response(response, show_sources)\n\n\n# Interactive test\nprint(\"üí¨ Interactive Movie Chatbot Demo\")\nprint(\"=\" * 50)\n\n# Example conversation\nqueries = [\n    \"What are some must-watch documentaries?\",\n    \"Recommend a movie with a rating above 8.0\"\n]\n\nfor q in queries:\n    print(f\"\\nüë§ You: {q}\")\n    print(chat_with_bot(q, show_sources=True))\n    print()",
   "metadata": {
    "id": "Irvou-hL_TtJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Optional: Test the functionality using a Gradio UI (intermediate check)\n\ndef simple_chat_interface(message: str, history: list) -> str:\n    \"\"\"\n    Simple chat function for Gradio interface.\n    \"\"\"\n    try:\n        response = ask_movie_bot(message)\n        return response['answer']\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create a simple Gradio chat interface for testing\nsimple_demo = gr.ChatInterface(\n    fn=simple_chat_interface,\n    title=\"üé¨ IMDb Movie Chatbot (Test Version)\",\n    description=\"Ask me anything about movies! I can recommend films, find movies by genre, director, or actors.\",\n    examples=[\n        \"Recommend a good documentary\",\n        \"What movies has Robert De Niro starred in?\",\n        \"Find me a highly rated biography film\",\n        \"What are some adventure movies from the 2000s?\",\n    ],\n    theme=\"soft\"\n)\n\n# Launch the test interface\n# Uncomment the line below to run the Gradio interface\n# simple_demo.launch(share=False)\n\nprint(\"‚úÖ Simple Gradio test interface created\")\nprint(\"   Uncomment 'simple_demo.launch()' to test the basic chatbot UI\")\nprint(\"   This is for intermediate testing before building the full agentic version\")",
   "metadata": {
    "id": "cy7cQ7KG_kSG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define various agents - each performing a particular task using tool decorator\n\n@tool\ndef search_movies_by_query(query: str) -> str:\n    \"\"\"\n    Search for movies based on a natural language query.\n    Use this tool when the user wants to find movies matching certain criteria.\n    \"\"\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n@tool\ndef get_movie_details(movie_title: str) -> str:\n    \"\"\"\n    Get detailed information about a specific movie by title.\n    Use this when the user asks about a specific movie.\n    \"\"\"\n    query = f\"Tell me everything about the movie titled {movie_title}\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n@tool\ndef recommend_movies_by_genre(genre: str) -> str:\n    \"\"\"\n    Recommend movies from a specific genre.\n    Use this when the user wants recommendations from a particular genre like Action, Comedy, Drama, etc.\n    \"\"\"\n    query = f\"Recommend the best {genre} movies with high ratings\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n@tool\ndef find_movies_by_actor(actor_name: str) -> str:\n    \"\"\"\n    Find movies featuring a specific actor.\n    Use this when the user wants to know what movies an actor has appeared in.\n    \"\"\"\n    query = f\"Find movies starring {actor_name}\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n@tool\ndef find_movies_by_director(director_name: str) -> str:\n    \"\"\"\n    Find movies by a specific director.\n    Use this when the user asks about movies from a particular director.\n    \"\"\"\n    query = f\"Find movies directed by {director_name}\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n@tool\ndef get_top_rated_movies(min_rating: float = 8.0) -> str:\n    \"\"\"\n    Get top rated movies above a certain IMDb rating threshold.\n    Use this when the user wants highly rated movies.\n    \"\"\"\n    query = f\"Find movies with IMDb rating above {min_rating}\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n@tool\ndef compare_movies(movie1: str, movie2: str) -> str:\n    \"\"\"\n    Compare two movies.\n    Use this when the user wants to compare different movies.\n    \"\"\"\n    query = f\"Compare the movies {movie1} and {movie2}. What are their ratings, genres, and key differences?\"\n    response = retrieval_chain.invoke({\"input\": query})\n    return response['answer']\n\n\n# ============================================================\n# POSTER HELPER FUNCTIONS\n# ============================================================\n\ndef get_movie_posters(movie_titles: List[str], max_posters: int = 5) -> List[Dict[str, str]]:\n    \"\"\"\n    Get poster URLs for a list of movie titles.\n    \n    Args:\n        movie_titles: List of movie titles to search for\n        max_posters: Maximum number of posters to return\n    \n    Returns:\n        List of dicts with title and poster_url\n    \"\"\"\n    posters = []\n    \n    for title in movie_titles[:max_posters]:\n        # Search for the movie in our vector store\n        results = vectorstore.similarity_search(f\"movie titled {title}\", k=1)\n        \n        if results:\n            doc = results[0]\n            poster_url = doc.metadata.get('poster_url', '')\n            \n            if poster_url and poster_url.strip():\n                posters.append({\n                    'title': doc.metadata.get('title', title),\n                    'year': doc.metadata.get('year', ''),\n                    'rating': doc.metadata.get('rating', ''),\n                    'genre': doc.metadata.get('genre', ''),\n                    'poster_url': poster_url\n                })\n    \n    logger.debug(f\"Found {len(posters)} posters for {len(movie_titles)} titles\")\n    return posters\n\n\ndef extract_movie_titles_from_response(response: str) -> List[str]:\n    \"\"\"\n    Extract movie titles from a chatbot response.\n    Uses simple heuristics to find movie titles.\n    \"\"\"\n    import re\n    \n    titles = []\n    \n    # Pattern 1: \"Movie Title\" (YEAR)\n    pattern1 = r'\"([^\"]+)\"\\s*\\((\\d{4})\\)'\n    matches1 = re.findall(pattern1, response)\n    titles.extend([m[0] for m in matches1])\n    \n    # Pattern 2: **Movie Title** (markdown bold)\n    pattern2 = r'\\*\\*([^*]+)\\*\\*'\n    matches2 = re.findall(pattern2, response)\n    titles.extend(matches2)\n    \n    # Pattern 3: Title (YEAR) - Rating\n    pattern3 = r'([A-Z][^.!?]*?)\\s*\\((\\d{4})\\)\\s*-'\n    matches3 = re.findall(pattern3, response)\n    titles.extend([m[0].strip() for m in matches3])\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_titles = []\n    for title in titles:\n        title_clean = title.strip()\n        if title_clean and title_clean.lower() not in seen:\n            seen.add(title_clean.lower())\n            unique_titles.append(title_clean)\n    \n    return unique_titles[:5]  # Return top 5 titles\n\n\ndef format_poster_gallery(posters: List[Dict[str, str]]) -> str:\n    \"\"\"\n    Format posters as HTML for Gradio display.\n    \"\"\"\n    if not posters:\n        return \"\"\n    \n    html = '<div style=\"display: flex; flex-wrap: wrap; gap: 15px; margin-top: 15px;\">'\n    \n    for p in posters:\n        html += f'''\n        <div style=\"text-align: center; width: 120px;\">\n            <img src=\"{p['poster_url']}\" \n                 alt=\"{p['title']}\" \n                 style=\"width: 100px; height: 150px; object-fit: cover; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.2);\"\n                 onerror=\"this.src='https://via.placeholder.com/100x150?text=No+Poster'\">\n            <p style=\"font-size: 11px; margin: 5px 0; font-weight: bold;\">{p['title'][:20]}{'...' if len(p['title']) > 20 else ''}</p>\n            <p style=\"font-size: 10px; margin: 0; color: #666;\">{p['year']} | ‚≠ê {p['rating']}</p>\n        </div>\n        '''\n    \n    html += '</div>'\n    return html\n\n\n# Collect all tools\ntools = [\n    search_movies_by_query,\n    get_movie_details,\n    recommend_movies_by_genre,\n    find_movies_by_actor,\n    find_movies_by_director,\n    get_top_rated_movies,\n    compare_movies,\n]\n\nprint(\"‚úÖ Movie Agent Tools Created:\")\nprint(\"-\" * 40)\nfor t in tools:\n    print(f\"  ‚Ä¢ {t.name}: {t.description[:60]}...\")\n\nprint(\"\\n‚úÖ Poster display functions created:\")\nprint(\"  ‚Ä¢ get_movie_posters: Retrieve poster URLs from database\")\nprint(\"  ‚Ä¢ extract_movie_titles_from_response: Parse titles from text\")\nprint(\"  ‚Ä¢ format_poster_gallery: Generate HTML gallery\")",
   "metadata": {
    "id": "k0ghQEvD_pcJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define the orchestrator logic to run the agents appropriately\nfrom langchain.memory import ConversationBufferMemory\n\n# Create the agent prompt\nAGENT_SYSTEM_PROMPT = \"\"\"You are a helpful Movie Recommendation Assistant powered by the IMDb database.\nYou have access to various tools to help users find and learn about movies.\n\nYour capabilities:\n- Search for movies by any criteria\n- Get details about specific movies\n- Recommend movies by genre\n- Find movies by actor or director\n- Find top-rated movies\n- Compare different movies\n\nAlways be helpful, conversational, and provide detailed movie information when relevant.\nIf you're not sure about something, use the appropriate tool to find the answer.\n\"\"\"\n\nagent_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", AGENT_SYSTEM_PROMPT),\n    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n    (\"human\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n])\n\n# Create the agent\nagent = create_openai_functions_agent(\n    llm=llm,\n    tools=tools,\n    prompt=agent_prompt\n)\n\n# Create conversation memory for personalization\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)\n\n# Create the agent executor (orchestrator)\n# Set verbose=True for debugging, False for production\nDEBUG_MODE = False  # Change to True to see agent reasoning\n\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    memory=memory,\n    verbose=DEBUG_MODE,\n    handle_parsing_errors=True,\n    max_iterations=5,\n)\n\nprint(\"‚úÖ Agent Orchestrator created\")\nprint(f\"   - Tools: {len(tools)} specialized movie tools\")\nprint(\"   - Memory: Conversation buffer for context\")\nprint(\"   - Max iterations: 5\")\nprint(f\"   - Debug mode: {DEBUG_MODE}\")\n\n# Test the agent\nprint(\"\\nüß™ Testing Agent Orchestrator...\")\nprint(\"-\" * 50)\n\ntest_result = agent_executor.invoke({\n    \"input\": \"Can you recommend some good documentaries?\"\n})\n\nprint(f\"\\nü§ñ Agent Response:\\n{test_result['output']}\")",
   "metadata": {
    "id": "QFhn7mcf_ziw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check the edge cases and handle them appropriately\nimport time\nimport hashlib\nimport json\nfrom collections import deque, OrderedDict\n\n# ============================================================\n# QUERY CACHE SYSTEM\n# ============================================================\n\nclass QueryCache:\n    \"\"\"\n    Intelligent query caching system with exact match and semantic similarity.\n    Reduces API costs by returning cached responses for similar queries.\n    \"\"\"\n    \n    def __init__(\n        self, \n        max_size: int = 100, \n        ttl_seconds: int = 3600,  # 1 hour default TTL\n        similarity_threshold: float = 0.92  # 92% similarity for semantic cache hit\n    ):\n        \"\"\"\n        Initialize the query cache.\n        \n        Args:\n            max_size: Maximum number of cached entries\n            ttl_seconds: Time-to-live for cache entries in seconds\n            similarity_threshold: Minimum cosine similarity for semantic cache hit\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.similarity_threshold = similarity_threshold\n        \n        # Exact match cache (query hash -> response)\n        self.exact_cache: OrderedDict = OrderedDict()\n        \n        # Semantic cache (stores embeddings for similarity matching)\n        self.semantic_cache: List[Dict] = []\n        \n        # Statistics\n        self.stats = {\n            \"exact_hits\": 0,\n            \"semantic_hits\": 0,\n            \"misses\": 0,\n            \"evictions\": 0\n        }\n        \n        logger.info(f\"QueryCache initialized: max_size={max_size}, ttl={ttl_seconds}s, threshold={similarity_threshold}\")\n    \n    def _hash_query(self, query: str) -> str:\n        \"\"\"Create a hash for the query string.\"\"\"\n        normalized = query.lower().strip()\n        return hashlib.md5(normalized.encode()).hexdigest()\n    \n    def _is_expired(self, timestamp: float) -> bool:\n        \"\"\"Check if a cache entry has expired.\"\"\"\n        return time.time() - timestamp > self.ttl_seconds\n    \n    def _evict_if_needed(self):\n        \"\"\"Evict oldest entries if cache is full.\"\"\"\n        while len(self.exact_cache) >= self.max_size:\n            self.exact_cache.popitem(last=False)\n            self.stats[\"evictions\"] += 1\n            logger.debug(\"Cache eviction performed (exact cache)\")\n        \n        while len(self.semantic_cache) >= self.max_size:\n            self.semantic_cache.pop(0)\n            self.stats[\"evictions\"] += 1\n            logger.debug(\"Cache eviction performed (semantic cache)\")\n    \n    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n        import numpy as np\n        vec1 = np.array(vec1)\n        vec2 = np.array(vec2)\n        \n        dot_product = np.dot(vec1, vec2)\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n        \n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n        \n        return dot_product / (norm1 * norm2)\n    \n    def get_exact(self, query: str) -> Optional[str]:\n        \"\"\"\n        Try to get an exact match from cache.\n        \n        Returns:\n            Cached response if found and not expired, None otherwise\n        \"\"\"\n        query_hash = self._hash_query(query)\n        \n        if query_hash in self.exact_cache:\n            entry = self.exact_cache[query_hash]\n            \n            if self._is_expired(entry[\"timestamp\"]):\n                # Entry expired, remove it\n                del self.exact_cache[query_hash]\n                logger.debug(f\"Cache entry expired for query hash: {query_hash[:8]}\")\n                return None\n            \n            # Move to end (LRU behavior)\n            self.exact_cache.move_to_end(query_hash)\n            self.stats[\"exact_hits\"] += 1\n            logger.info(f\"Exact cache hit for query: {query[:50]}...\")\n            return entry[\"response\"]\n        \n        return None\n    \n    def get_semantic(self, query: str, query_embedding: List[float]) -> Optional[str]:\n        \"\"\"\n        Try to find a semantically similar cached query.\n        \n        Args:\n            query: The query string\n            query_embedding: The embedding vector of the query\n            \n        Returns:\n            Cached response if similar query found, None otherwise\n        \"\"\"\n        best_match = None\n        best_similarity = 0.0\n        expired_indices = []\n        \n        for i, entry in enumerate(self.semantic_cache):\n            # Check expiration\n            if self._is_expired(entry[\"timestamp\"]):\n                expired_indices.append(i)\n                continue\n            \n            # Calculate similarity\n            similarity = self._cosine_similarity(query_embedding, entry[\"embedding\"])\n            \n            if similarity > best_similarity:\n                best_similarity = similarity\n                best_match = entry\n        \n        # Clean up expired entries\n        for i in reversed(expired_indices):\n            self.semantic_cache.pop(i)\n        \n        # Check if we have a good enough match\n        if best_match and best_similarity >= self.similarity_threshold:\n            self.stats[\"semantic_hits\"] += 1\n            logger.info(f\"Semantic cache hit (similarity: {best_similarity:.3f}) for: {query[:50]}...\")\n            return best_match[\"response\"]\n        \n        return None\n    \n    def set(self, query: str, response: str, query_embedding: Optional[List[float]] = None):\n        \"\"\"\n        Store a query-response pair in the cache.\n        \n        Args:\n            query: The query string\n            response: The response to cache\n            query_embedding: Optional embedding for semantic caching\n        \"\"\"\n        self._evict_if_needed()\n        \n        timestamp = time.time()\n        query_hash = self._hash_query(query)\n        \n        # Store in exact cache\n        self.exact_cache[query_hash] = {\n            \"query\": query,\n            \"response\": response,\n            \"timestamp\": timestamp\n        }\n        \n        # Store in semantic cache if embedding provided\n        if query_embedding is not None:\n            self.semantic_cache.append({\n                \"query\": query,\n                \"embedding\": query_embedding,\n                \"response\": response,\n                \"timestamp\": timestamp\n            })\n        \n        logger.debug(f\"Cached response for query: {query[:50]}...\")\n    \n    def clear(self):\n        \"\"\"Clear all cache entries.\"\"\"\n        self.exact_cache.clear()\n        self.semantic_cache.clear()\n        logger.info(\"Query cache cleared\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Get cache statistics.\"\"\"\n        total_requests = self.stats[\"exact_hits\"] + self.stats[\"semantic_hits\"] + self.stats[\"misses\"]\n        hit_rate = (self.stats[\"exact_hits\"] + self.stats[\"semantic_hits\"]) / max(total_requests, 1) * 100\n        \n        return {\n            \"exact_cache_size\": len(self.exact_cache),\n            \"semantic_cache_size\": len(self.semantic_cache),\n            \"exact_hits\": self.stats[\"exact_hits\"],\n            \"semantic_hits\": self.stats[\"semantic_hits\"],\n            \"total_hits\": self.stats[\"exact_hits\"] + self.stats[\"semantic_hits\"],\n            \"misses\": self.stats[\"misses\"],\n            \"evictions\": self.stats[\"evictions\"],\n            \"hit_rate_percent\": round(hit_rate, 1)\n        }\n\n\n# ============================================================\n# RATE LIMITER\n# ============================================================\n\nclass RateLimiter:\n    \"\"\"\n    Simple rate limiter using a sliding window approach.\n    \"\"\"\n    \n    def __init__(self, max_requests: int = 10, window_seconds: int = 60):\n        \"\"\"\n        Initialize rate limiter.\n        \n        Args:\n            max_requests: Maximum number of requests allowed in the time window\n            window_seconds: Time window in seconds\n        \"\"\"\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.requests = deque()\n        logger.info(f\"Rate limiter initialized: {max_requests} requests per {window_seconds}s\")\n    \n    def is_allowed(self) -> Tuple[bool, Optional[float]]:\n        \"\"\"\n        Check if a request is allowed under the rate limit.\n        \n        Returns:\n            Tuple of (is_allowed, wait_time_if_not_allowed)\n        \"\"\"\n        now = time.time()\n        \n        # Remove expired timestamps\n        while self.requests and self.requests[0] < now - self.window_seconds:\n            self.requests.popleft()\n        \n        if len(self.requests) < self.max_requests:\n            self.requests.append(now)\n            return True, None\n        else:\n            # Calculate wait time until oldest request expires\n            wait_time = self.requests[0] + self.window_seconds - now\n            logger.warning(f\"Rate limit exceeded. Wait time: {wait_time:.1f}s\")\n            return False, wait_time\n    \n    def get_remaining(self) -> int:\n        \"\"\"Get remaining requests in current window.\"\"\"\n        now = time.time()\n        while self.requests and self.requests[0] < now - self.window_seconds:\n            self.requests.popleft()\n        return self.max_requests - len(self.requests)\n\n\n# ============================================================\n# MOVIE CHATBOT CLASS (with Caching)\n# ============================================================\n\nclass MovieChatbot:\n    \"\"\"\n    A robust movie chatbot class with edge case handling, logging, rate limiting, and caching.\n    \"\"\"\n    \n    def __init__(\n        self, \n        agent_executor, \n        rate_limit: int = 20, \n        rate_window: int = 60,\n        cache_size: int = 100,\n        cache_ttl: int = 3600,\n        enable_semantic_cache: bool = True\n    ):\n        \"\"\"\n        Initialize the movie chatbot.\n        \n        Args:\n            agent_executor: The LangChain agent executor\n            rate_limit: Max requests per time window (default: 20)\n            rate_window: Time window in seconds (default: 60)\n            cache_size: Max cached queries (default: 100)\n            cache_ttl: Cache TTL in seconds (default: 3600 = 1 hour)\n            enable_semantic_cache: Enable semantic similarity caching (default: True)\n        \"\"\"\n        self.agent = agent_executor\n        self.conversation_history = []\n        self.request_count = 0\n        self.session_start = time.time()\n        self.rate_limiter = RateLimiter(max_requests=rate_limit, window_seconds=rate_window)\n        self.cache = QueryCache(max_size=cache_size, ttl_seconds=cache_ttl)\n        self.enable_semantic_cache = enable_semantic_cache\n        \n        logger.info(\"MovieChatbot instance initialized with caching\")\n        \n    def _validate_input(self, user_input: str) -> Tuple[bool, str]:\n        \"\"\"\n        Validate user input and return (is_valid, error_message).\n        \"\"\"\n        # Check for empty input\n        if not user_input or not user_input.strip():\n            logger.warning(\"Validation failed: Empty input received\")\n            return False, \"Please enter a question or request about movies.\"\n        \n        # Check for very short input\n        if len(user_input.strip()) < 3:\n            logger.warning(f\"Validation failed: Input too short ({len(user_input.strip())} chars)\")\n            return False, \"Please provide a more detailed question.\"\n        \n        # Check for very long input (potential abuse)\n        if len(user_input) > 1000:\n            logger.warning(f\"Validation failed: Input too long ({len(user_input)} chars)\")\n            return False, \"Your question is too long. Please keep it under 1000 characters.\"\n        \n        return True, \"\"\n    \n    def _get_query_embedding(self, query: str) -> Optional[List[float]]:\n        \"\"\"Get embedding for a query (for semantic caching).\"\"\"\n        try:\n            return embeddings.embed_query(query)\n        except Exception as e:\n            logger.warning(f\"Failed to get embedding: {e}\")\n            return None\n    \n    def chat(self, user_input: str, use_cache: bool = True) -> str:\n        \"\"\"\n        Process user input with comprehensive error handling, logging, rate limiting, and caching.\n        \n        Args:\n            user_input: The user's query\n            use_cache: Whether to use caching (default: True)\n        \"\"\"\n        self.request_count += 1\n        request_id = f\"REQ-{self.request_count:04d}\"\n        start_time = time.time()\n        \n        logger.info(f\"[{request_id}] New request received\")\n        logger.debug(f\"[{request_id}] Input: {user_input[:100]}{'...' if len(user_input) > 100 else ''}\")\n        \n        # Validate input first (before checking cache/rate limit)\n        is_valid, error_msg = self._validate_input(user_input)\n        if not is_valid:\n            logger.info(f\"[{request_id}] Request rejected: {error_msg}\")\n            return f\"‚ö†Ô∏è {error_msg}\"\n        \n        # Try exact cache first\n        if use_cache:\n            cached_response = self.cache.get_exact(user_input)\n            if cached_response:\n                logger.info(f\"[{request_id}] Exact cache hit - returning cached response\")\n                return cached_response\n            \n            # Try semantic cache\n            if self.enable_semantic_cache:\n                query_embedding = self._get_query_embedding(user_input)\n                if query_embedding:\n                    cached_response = self.cache.get_semantic(user_input, query_embedding)\n                    if cached_response:\n                        logger.info(f\"[{request_id}] Semantic cache hit - returning cached response\")\n                        return cached_response\n        \n        # Cache miss - check rate limit before making API call\n        is_allowed, wait_time = self.rate_limiter.is_allowed()\n        if not is_allowed:\n            logger.warning(f\"[{request_id}] Rate limited. Wait time: {wait_time:.1f}s\")\n            return f\"‚ö†Ô∏è Too many requests. Please wait {wait_time:.0f} seconds before trying again. (Remaining: {self.rate_limiter.get_remaining()})\"\n        \n        # Cache miss recorded\n        self.cache.stats[\"misses\"] += 1\n        \n        try:\n            # Process through agent\n            logger.debug(f\"[{request_id}] Cache miss - invoking agent...\")\n            response = self.agent.invoke({\"input\": user_input})\n            \n            # Calculate response time\n            response_time = time.time() - start_time\n            \n            # Store in history for context\n            self.conversation_history.append({\n                \"request_id\": request_id,\n                \"timestamp\": datetime.now().isoformat(),\n                \"user\": user_input,\n                \"assistant\": response['output'],\n                \"response_time\": response_time,\n                \"cache_hit\": False\n            })\n            \n            # Cache the response\n            if use_cache:\n                query_embedding = self._get_query_embedding(user_input) if self.enable_semantic_cache else None\n                self.cache.set(user_input, response['output'], query_embedding)\n            \n            logger.info(f\"[{request_id}] Request completed in {response_time:.2f}s (cached)\")\n            logger.debug(f\"[{request_id}] Response length: {len(response['output'])} chars\")\n            \n            return response['output']\n            \n        except Exception as e:\n            # Calculate error response time\n            error_time = time.time() - start_time\n            error_type = type(e).__name__\n            \n            logger.error(f\"[{request_id}] Error after {error_time:.2f}s: {error_type} - {str(e)}\")\n            \n            if \"RateLimitError\" in error_type:\n                logger.warning(f\"[{request_id}] OpenAI rate limit exceeded\")\n                return \"‚ö†Ô∏è I'm receiving too many requests. Please wait a moment and try again.\"\n            elif \"AuthenticationError\" in error_type:\n                logger.critical(f\"[{request_id}] Authentication error - check API key\")\n                return \"‚ö†Ô∏è There's an issue with the API configuration. Please check your API key.\"\n            elif \"Timeout\" in error_type:\n                logger.warning(f\"[{request_id}] Request timed out\")\n                return \"‚ö†Ô∏è The request timed out. Please try again with a simpler question.\"\n            else:\n                logger.error(f\"[{request_id}] Unhandled error: {str(e)}\")\n                return f\"‚ö†Ô∏è An error occurred: {str(e)}. Please try rephrasing your question.\"\n    \n    def clear_history(self):\n        \"\"\"Clear conversation history.\"\"\"\n        history_size = len(self.conversation_history)\n        self.conversation_history = []\n        self.agent.memory.clear()\n        logger.info(f\"Conversation history cleared ({history_size} messages removed)\")\n        return \"Conversation history cleared!\"\n    \n    def clear_cache(self):\n        \"\"\"Clear the query cache.\"\"\"\n        self.cache.clear()\n        return \"Query cache cleared!\"\n    \n    def get_history(self) -> List[Dict]:\n        \"\"\"Get conversation history.\"\"\"\n        return self.conversation_history\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Get session statistics including cache stats.\"\"\"\n        session_duration = time.time() - self.session_start\n        cache_stats = self.cache.get_stats()\n        \n        stats = {\n            \"session_duration_seconds\": round(session_duration, 2),\n            \"total_requests\": self.request_count,\n            \"conversation_turns\": len(self.conversation_history),\n            \"avg_response_time\": round(\n                sum(h.get('response_time', 0) for h in self.conversation_history) / \n                max(len(self.conversation_history), 1), 2\n            ),\n            \"rate_limit_remaining\": self.rate_limiter.get_remaining(),\n            \"cache\": cache_stats\n        }\n        logger.debug(f\"Session stats: {stats}\")\n        return stats\n\n\n# Create the chatbot instance with caching enabled\n# - Rate limit: 20 requests per minute\n# - Cache: 100 queries, 1 hour TTL\n# - Semantic caching: Enabled\nchatbot = MovieChatbot(\n    agent_executor, \n    rate_limit=20, \n    rate_window=60,\n    cache_size=100,\n    cache_ttl=3600,\n    enable_semantic_cache=True\n)\n\n# Test edge cases\nprint(\"üß™ Testing Edge Cases and Caching\")\nprint(\"=\" * 50)\n\nedge_cases = [\n    \"\",  # Empty input\n    \"hi\",  # Too short\n    \"Recommend some good documentaries about nature\",  # Valid query\n    \"Recommend some good documentaries about nature\",  # Should hit cache!\n    \"What's the weather like?\",  # Non-movie query (LLM should handle gracefully)\n]\n\nfor test_input in edge_cases:\n    print(f\"\\nüìù Input: '{test_input}'\")\n    print(f\"ü§ñ Response: {chatbot.chat(test_input)[:150]}...\")\n    print(\"-\" * 40)\n\nprint(\"\\n‚úÖ Edge case handling, rate limiting, and caching implemented\")\nprint(f\"\\nüìä Session Stats:\")\nstats = chatbot.get_stats()\nprint(f\"   Requests: {stats['total_requests']}\")\nprint(f\"   Cache Hits: {stats['cache']['total_hits']} (Exact: {stats['cache']['exact_hits']}, Semantic: {stats['cache']['semantic_hits']})\")\nprint(f\"   Cache Misses: {stats['cache']['misses']}\")\nprint(f\"   Hit Rate: {stats['cache']['hit_rate_percent']}%\")",
   "metadata": {
    "id": "4uzZuR1VAKhu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create a UI using gradio or any other tool of your choice\nfrom typing import Iterator, Tuple\nimport re\n\n# Store last response for poster extraction\nlast_response_cache = {\"response\": \"\", \"posters_html\": \"\"}\n\n\ndef agentic_chat(message: str, history: list) -> str:\n    \"\"\"\n    Main chat function for the agentic Gradio interface.\n    Uses the MovieChatbot class with full agent capabilities.\n    \"\"\"\n    return chatbot.chat(message)\n\n\ndef agentic_chat_with_posters(message: str, history: list) -> Tuple[str, str]:\n    \"\"\"\n    Chat function that also returns movie posters.\n    \n    Returns:\n        Tuple of (response_text, posters_html)\n    \"\"\"\n    # Get the chatbot response\n    response = chatbot.chat(message)\n    \n    # Extract movie titles and get posters\n    try:\n        titles = extract_movie_titles_from_response(response)\n        if titles:\n            posters = get_movie_posters(titles)\n            posters_html = format_poster_gallery(posters)\n        else:\n            posters_html = \"\"\n    except Exception as e:\n        logger.warning(f\"Error extracting posters: {e}\")\n        posters_html = \"\"\n    \n    # Cache for later use\n    last_response_cache[\"response\"] = response\n    last_response_cache[\"posters_html\"] = posters_html\n    \n    return response, posters_html\n\n\ndef agentic_chat_streaming(message: str, history: list) -> Iterator[str]:\n    \"\"\"\n    Streaming chat function that yields partial responses.\n    Provides real-time feedback to users.\n    \"\"\"\n    # First validate input\n    is_valid, error_msg = chatbot._validate_input(message)\n    if not is_valid:\n        yield f\"‚ö†Ô∏è {error_msg}\"\n        return\n    \n    # Show thinking indicator\n    yield \"ü§î Thinking...\"\n    \n    try:\n        # Get full response (uses caching internally)\n        response = chatbot.chat(message)\n        \n        # Extract and cache posters for display\n        try:\n            titles = extract_movie_titles_from_response(response)\n            if titles:\n                posters = get_movie_posters(titles)\n                last_response_cache[\"posters_html\"] = format_poster_gallery(posters)\n            else:\n                last_response_cache[\"posters_html\"] = \"\"\n        except Exception as e:\n            logger.warning(f\"Error extracting posters: {e}\")\n            last_response_cache[\"posters_html\"] = \"\"\n        \n        last_response_cache[\"response\"] = response\n        \n        # Simulate streaming by yielding chunks\n        words = response.split()\n        partial = \"\"\n        \n        for i, word in enumerate(words):\n            partial += word + \" \"\n            if i % 3 == 0:\n                yield partial\n        \n        # Yield final complete response\n        yield response\n        \n    except Exception as e:\n        logger.error(f\"Streaming error: {str(e)}\")\n        yield f\"‚ö†Ô∏è An error occurred: {str(e)}\"\n\n\ndef get_cached_posters():\n    \"\"\"Return the cached posters HTML from the last response.\"\"\"\n    return last_response_cache.get(\"posters_html\", \"\")\n\n\ndef clear_conversation():\n    \"\"\"Clear the chatbot's conversation history.\"\"\"\n    last_response_cache[\"response\"] = \"\"\n    last_response_cache[\"posters_html\"] = \"\"\n    return chatbot.clear_history()\n\n\ndef clear_query_cache():\n    \"\"\"Clear the query cache.\"\"\"\n    return chatbot.clear_cache()\n\n\ndef get_session_stats():\n    \"\"\"Get current session statistics including cache info.\"\"\"\n    stats = chatbot.get_stats()\n    cache = stats.get('cache', {})\n    \n    return f\"\"\"üìä **Session Statistics**\n\n**Requests:**\n- Total: {stats['total_requests']}\n- Conversation Turns: {stats['conversation_turns']}\n- Avg Response Time: {stats['avg_response_time']:.2f}s\n- Rate Limit Remaining: {stats['rate_limit_remaining']}\n\n**Cache Performance:**\n- Hit Rate: {cache.get('hit_rate_percent', 0)}%\n- Exact Hits: {cache.get('exact_hits', 0)}\n- Semantic Hits: {cache.get('semantic_hits', 0)}\n- Misses: {cache.get('misses', 0)}\n- Cache Size: {cache.get('exact_cache_size', 0)} queries\n\n**Session Duration:** {stats['session_duration_seconds']:.0f}s\"\"\"\n\n\n# Create the full-featured Gradio interface with streaming and posters\nwith gr.Blocks(\n    title=\"IMDb Movie Chatbot\",\n    theme=gr.themes.Soft(),\n    css=\"\"\"\n    .gradio-container {max-width: 1000px !important}\n    .message {font-size: 16px !important}\n    .stats-box {background: #f0f4f8; padding: 10px; border-radius: 8px; margin-top: 10px;}\n    .poster-gallery {background: #fafafa; padding: 15px; border-radius: 10px; border: 1px solid #eee;}\n    .cache-info {background: #e8f5e9; padding: 8px; border-radius: 6px; font-size: 12px;}\n    \"\"\"\n) as demo:\n    \n    # Header\n    gr.Markdown(\"\"\"\n    # üé¨ IMDb Movie Chatbot\n    ### Your AI-powered Movie Discovery Assistant\n    \n    Ask me anything about movies! I can help you:\n    - üéØ **Find movies** by genre, actor, director, or any criteria\n    - ‚≠ê **Get recommendations** based on your preferences  \n    - üìä **Compare movies** and get detailed information\n    - üé≠ **Discover** highly-rated films and hidden gems\n    - üñºÔ∏è **View posters** of recommended movies\n    - ‚ö° **Fast responses** via intelligent query caching\n    \n    ---\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column(scale=2):\n            # Main chat interface with streaming\n            chat_interface = gr.ChatInterface(\n                fn=agentic_chat_streaming,\n                examples=[\n                    \"Recommend some highly rated documentaries\",\n                    \"Find movies starring Tom Hanks\",\n                    \"What are some good adventure movies from the 2000s?\",\n                    \"Tell me about biography films with ratings above 7.5\",\n                    \"Compare documentary and biography genres\",\n                    \"What movies are directed by Martin Scorsese?\",\n                ],\n                retry_btn=\"üîÑ Retry\",\n                undo_btn=\"‚Ü©Ô∏è Undo\",\n                clear_btn=\"üóëÔ∏è Clear\",\n            )\n        \n        with gr.Column(scale=1):\n            # Poster gallery sidebar\n            gr.Markdown(\"### üñºÔ∏è Movie Posters\")\n            poster_display = gr.HTML(\n                value=\"<p style='color: #888; text-align: center;'>Posters will appear here after you ask about movies!</p>\",\n                elem_classes=[\"poster-gallery\"]\n            )\n            \n            # Button to refresh posters\n            refresh_posters_btn = gr.Button(\"üîÑ Show Posters\", size=\"sm\")\n            refresh_posters_btn.click(\n                fn=get_cached_posters,\n                inputs=[],\n                outputs=[poster_display]\n            )\n            \n            # Stats section\n            gr.Markdown(\"### üìä Session Stats\")\n            stats_output = gr.Markdown(\"Click button below to view stats\", elem_classes=[\"stats-box\"])\n            \n            with gr.Row():\n                stats_btn = gr.Button(\"üìä Refresh\", size=\"sm\")\n                clear_cache_btn = gr.Button(\"üóëÔ∏è Clear Cache\", size=\"sm\")\n            \n            stats_btn.click(fn=get_session_stats, inputs=[], outputs=[stats_output])\n            clear_cache_btn.click(fn=clear_query_cache, inputs=[], outputs=[stats_output])\n    \n    # Footer with additional info\n    gr.Markdown(\"\"\"\n    ---\n    ### üìö About This Chatbot\n    \n    This chatbot uses:\n    - **RAG (Retrieval-Augmented Generation)** for accurate movie information\n    - **FAISS Vector Search** for fast similarity matching\n    - **OpenAI GPT** for natural language understanding\n    - **Agentic AI Architecture** with 7 specialized movie tools\n    - **Real-time Streaming** for responsive user experience\n    - **Rate Limiting** for fair usage (20 requests/minute)\n    - **Intelligent Caching** with exact match + semantic similarity\n    - **Movie Posters** from the IMDb dataset\n    \n    **Dataset:** IMDb Movie Database with 3,000+ movies\n    \n    ---\n    *Built with LangChain, FAISS, and Gradio*\n    \"\"\")\n\n# Launch instructions\nprint(\"=\" * 60)\nprint(\"üé¨ IMDb Movie Chatbot - Full Version with Caching\")\nprint(\"=\" * 60)\nprint(\"\\n‚úÖ Gradio interface created successfully!\")\nprint(\"   Features:\")\nprint(\"   - Streaming responses\")\nprint(\"   - Rate limiting (20 req/min)\")\nprint(\"   - Query caching (exact + semantic)\")\nprint(\"   - Session statistics\")\nprint(\"   - Movie poster display\")\nprint(\"\\nüìå To launch the chatbot, uncomment and run:\")\nprint(\"   demo.launch(share=False)\")\nprint(\"\\nüìå For public sharing:\")\nprint(\"   demo.launch(share=True)\")\nprint(\"\\n\" + \"=\" * 60)\n\n# Uncomment to launch:\n# demo.launch(share=False)",
   "metadata": {
    "id": "JV2FZaosAVLj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 19: TEST SUITE - Run this to test the chatbot\n\n# ============================================================\n# TEST CASES\n# ============================================================\n\nTEST_CASES = {\n    \"Basic Functionality\": [\n        (\"BF001\", \"Genre Search\", \"Recommend some comedy movies\"),\n        (\"BF002\", \"Genre Search\", \"Find documentary films\"),\n        (\"BF003\", \"Actor Search\", \"What movies has Tom Hanks starred in?\"),\n        (\"BF004\", \"Director Search\", \"Find movies directed by Steven Spielberg\"),\n        (\"BF005\", \"Rating Filter\", \"Show movies rated above 8.0\"),\n        (\"BF006\", \"Year Filter\", \"What movies came out in 2020?\"),\n    ],\n    \"Complex Queries\": [\n        (\"CQ001\", \"Multi-criteria\", \"Find a documentary about music with good ratings\"),\n        (\"CQ002\", \"Time Period\", \"What adventure movies from the 90s should I watch?\"),\n        (\"CQ003\", \"Comparison\", \"Compare documentary and biography genres\"),\n        (\"CQ004\", \"Recommendation\", \"Recommend highly rated biography films\"),\n    ],\n    \"Edge Cases\": [\n        (\"EC001\", \"Empty Input\", \"\"),\n        (\"EC002\", \"Short Input\", \"hi\"),\n        (\"EC003\", \"Non-movie\", \"What's the weather like?\"),\n        (\"EC004\", \"Misspelled\", \"Recomend comdy moveis\"),\n        (\"EC005\", \"Not Found\", \"Tell me about movie XYZ123ABC\"),\n    ],\n}\n\ndef run_tests(chatbot, verbose=True):\n    \"\"\"Run all test cases and report results.\"\"\"\n    print(\"=\" * 60)\n    print(\"üß™ IMDb MOVIE CHATBOT - TEST SUITE\")\n    print(\"=\" * 60)\n    \n    total = 0\n    passed = 0\n    \n    for category, tests in TEST_CASES.items():\n        print(f\"\\nüìÅ {category}\")\n        print(\"-\" * 40)\n        \n        for test_id, test_type, query in tests:\n            total += 1\n            \n            try:\n                response = chatbot.chat(query)\n                \n                # Check if response is valid (not an error for valid queries)\n                if query == \"\":  # Empty should return error\n                    success = \"‚ö†Ô∏è\" in response\n                elif len(query) < 3:  # Short should return error\n                    success = \"‚ö†Ô∏è\" in response\n                else:  # Others should return actual content\n                    success = len(response) > 20 and \"‚ö†Ô∏è\" not in response\n                \n                if success:\n                    passed += 1\n                    status = \"‚úÖ PASS\"\n                else:\n                    status = \"‚ùå FAIL\"\n                \n                print(f\"\\n[{test_id}] {test_type}: {status}\")\n                print(f\"    Query: \\\"{query[:40]}{'...' if len(query) > 40 else ''}\\\"\")\n                \n                if verbose:\n                    print(f\"    Response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n                    \n            except Exception as e:\n                print(f\"\\n[{test_id}] {test_type}: üí• ERROR\")\n                print(f\"    Error: {str(e)}\")\n    \n    # Summary\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"üìä RESULTS: {passed}/{total} tests passed ({100*passed/total:.1f}%)\")\n    print(\"=\" * 60)\n    \n    return passed, total\n\n# Run the tests\nprint(\"Running test suite...\\n\")\npassed, total = run_tests(chatbot, verbose=True)\n\n# Quick test examples for manual verification\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üìù ADDITIONAL MANUAL TEST QUERIES\")\nprint(\"=\" * 60)\nprint(\"\"\"\nTry these queries manually in the Gradio UI:\n\n1. \"What are the top 5 highest rated movies?\"\n2. \"Find horror movies from the 2010s\"\n3. \"Movies similar to documentaries about sports\"\n4. \"Who directed the movie Inception?\"\n5. \"List all movies with rating above 8.5\"\n6. \"Compare action and adventure genres\"\n7. \"Find movies less than 90 minutes long\"\n8. \"What PG-13 movies are available?\"\n\"\"\")",
   "metadata": {
    "id": "MBOI1-LrAjum"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Test Documentation & Performance Analysis\n\n## Test Categories\n\n### 1. Basic Functionality Tests (BF001-BF006)\n| ID | Type | Query | Pass Criteria |\n|----|------|-------|---------------|\n| BF001 | Genre Search | \"Recommend some comedy movies\" | Returns comedy movies |\n| BF002 | Genre Search | \"Find documentary films\" | Returns documentaries |\n| BF003 | Actor Search | \"Movies with Tom Hanks\" | Returns actor's films |\n| BF004 | Director Search | \"Films by Steven Spielberg\" | Returns director's work |\n| BF005 | Rating Filter | \"Movies rated above 8.0\" | Returns high-rated films |\n| BF006 | Year Filter | \"Movies from 2020\" | Returns 2020 releases |\n\n### 2. Complex Query Tests (CQ001-CQ004)\n| ID | Type | Query | Pass Criteria |\n|----|------|-------|---------------|\n| CQ001 | Multi-criteria | \"Documentary about music with good ratings\" | Combines genre + topic + rating |\n| CQ002 | Time Period | \"Adventure movies from the 90s\" | Filters by decade |\n| CQ003 | Comparison | \"Compare documentary and biography\" | Compares two genres |\n| CQ004 | Recommendation | \"Highly rated biography films\" | Quality recommendations |\n\n### 3. Edge Case Tests (EC001-EC005)\n| ID | Type | Query | Expected Behavior |\n|----|------|-------|-------------------|\n| EC001 | Empty Input | \"\" | Error message |\n| EC002 | Short Input | \"hi\" | Ask for more detail |\n| EC003 | Non-movie | \"What's the weather?\" | Graceful handling |\n| EC004 | Misspelled | \"Recomend comdy\" | Still finds results |\n| EC005 | Not Found | \"Movie XYZ123ABC\" | Says not found |\n\n## Performance Metrics\n\n| Metric | Target | How to Measure |\n|--------|--------|----------------|\n| Response Time | < 5 seconds | Time from query to response |\n| Retrieval Accuracy | Top 5 relevant | Check if results match query |\n| Error Handling | 100% graceful | No crashes on edge cases |\n| Conversation Memory | Maintains context | Follow-up questions work |\n\n## Future Test Additions\n- [ ] Load testing (multiple concurrent users)\n- [ ] Response consistency (same query = similar results)\n- [ ] Latency benchmarking\n- [ ] Token usage optimization\n- [ ] Multimodal tests (when poster display added)",
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}